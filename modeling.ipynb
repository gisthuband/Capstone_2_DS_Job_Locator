{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01eb9f9",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "The goal of this model is to generate a list of potential cities, companies, and job titles fitting my desired salary range and perceived competitiveness in the job market.\n",
    "\n",
    "Inputs: a desired salary range, and self perceived competitiveness in the job market\n",
    "\n",
    "Outputs: cities and companies to look at applying to, alongside with job titles\n",
    "\n",
    "Dataframe Modelling Features: upper salary offer, lower salary offer, company rating, total data science related positions in the state of company in 2023, ratio of new postings in state to total data science positions, annual mean and median wage of data science professionals in company's state, and ratio of jobs offering pay to state's mean and median wages.\n",
    "\n",
    "\n",
    "Using a dataset detailing the job postings within the data science market in 2024 (https://www.kaggle.com/datasets/ritiksharma07/data-science-job-listings-from-glassdoor) and a dataset from the Bureau of labor statistics on data science salary and employment stats in 2023 (https://data.bls.gov/oes/#/occInd/One%20occupation%20for%20multiple%20industries), I was able to construct a dataframe with several hundred jobs posted, their upper and lower salaries in pay, the company rating, the amount of data science related positions that are within the state of the job, the annual mean wage of data science related professionals in the company's state, the median wage of data science related professional in the company's state, the ratio of the jobs pay to to the state mean and median, and the relative ratio of new data science job postings to total positions already in the state.  The jobs were labelled according to which region of the country that are in; the region being symbolic of the economy there.\n",
    "\n",
    "Using these numerical features, I was able to test out three classification models: random forest classification, K neighbors classifications, and gradient boosting classifications.  The random forest and gradient boosting classifiers were able to perform the best as far as accuracy and F1 scores go, so those two hyper parameter tuned models will be used in this modelling.\n",
    "\n",
    "The model will be inserted into a function that will take into account, the salary range and perceived competitiveness (perceived competitiveness is best explained as: if you gave your self a 1.1 that means that you believe you deserve pay above the average/median pay of data science related professionals of the region; conversely a rating of .8 would indicate that you believe you deserve pay below the average/median pay of data science related professionals of the region).\n",
    "\n",
    "The previous installments in this projects progression can be found in these links:\n",
    "https://github.com/gisthuband/Capstone_2_DS_Job_Locator/blob/main/data_wrangle.ipynb https://github.com/gisthuband/Capstone_2_DS_Job_Locator/blob/main/exploratory_data_analysis.ipynb\n",
    "https://github.com/gisthuband/Capstone_2_DS_Job_Locator/blob/main/preprocessing%20and%20model%20training.ipynb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This notebook will contain the following:\n",
    "\n",
    "1.) Modeling and prediction with the hyperparameter tuned random forest classifier\n",
    "\n",
    "2.) Modeling and prediction with the hyperparameter tuned gradient boosted classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3134c7",
   "metadata": {},
   "source": [
    "# Imports and Dataframe Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c7fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score,log_loss\n",
    "from matplotlib import pyplot\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758bf01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>upper_salary</th>\n",
       "      <th>lower_salary</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company Rating</th>\n",
       "      <th>company_name</th>\n",
       "      <th>tot_employment_in_state</th>\n",
       "      <th>Annual mean wage(2)</th>\n",
       "      <th>Annual median wage(2)</th>\n",
       "      <th>total_new_post_rat</th>\n",
       "      <th>range_avg_to_mean_ratio</th>\n",
       "      <th>range_avg_to_median_ratio</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84000.000000</td>\n",
       "      <td>57000.000000</td>\n",
       "      <td>WI</td>\n",
       "      <td>Onalaska</td>\n",
       "      <td>Associate Stop Loss Underwriter</td>\n",
       "      <td>2.7</td>\n",
       "      <td>The Insurance Center</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>105250.0</td>\n",
       "      <td>101850.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>0.669834</td>\n",
       "      <td>0.692194</td>\n",
       "      <td>midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148165.491991</td>\n",
       "      <td>104355.331808</td>\n",
       "      <td>WI</td>\n",
       "      <td>Eau Claire</td>\n",
       "      <td>Marketing Advertising Analyst</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Net Health Shops, LLC</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>105250.0</td>\n",
       "      <td>101850.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>1.199624</td>\n",
       "      <td>1.239670</td>\n",
       "      <td>midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160000.000000</td>\n",
       "      <td>135000.000000</td>\n",
       "      <td>WI</td>\n",
       "      <td>Madison</td>\n",
       "      <td>Manager - IT Infrastructure Engineering</td>\n",
       "      <td>3.9</td>\n",
       "      <td>UW Credit Union</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>105250.0</td>\n",
       "      <td>101850.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>1.401425</td>\n",
       "      <td>1.448208</td>\n",
       "      <td>midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84000.000000</td>\n",
       "      <td>59000.000000</td>\n",
       "      <td>WI</td>\n",
       "      <td>Wausau</td>\n",
       "      <td>Associate Stop Loss Underwriter</td>\n",
       "      <td>2.7</td>\n",
       "      <td>The Insurance Center</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>105250.0</td>\n",
       "      <td>101850.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>0.679335</td>\n",
       "      <td>0.702013</td>\n",
       "      <td>midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87000.000000</td>\n",
       "      <td>58000.000000</td>\n",
       "      <td>WI</td>\n",
       "      <td>New Berlin</td>\n",
       "      <td>Supply Chain Data Analyst (Day Shift) - New Be...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>DB SCHENKER</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>105250.0</td>\n",
       "      <td>101850.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>0.688836</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>midwest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    upper_salary   lower_salary state        city  \\\n",
       "0   84000.000000   57000.000000    WI    Onalaska   \n",
       "1  148165.491991  104355.331808    WI  Eau Claire   \n",
       "2  160000.000000  135000.000000    WI     Madison   \n",
       "3   84000.000000   59000.000000    WI      Wausau   \n",
       "4   87000.000000   58000.000000    WI  New Berlin   \n",
       "\n",
       "                                           Job Title  Company Rating  \\\n",
       "0                    Associate Stop Loss Underwriter             2.7   \n",
       "1                      Marketing Advertising Analyst             3.0   \n",
       "2            Manager - IT Infrastructure Engineering             3.9   \n",
       "3                    Associate Stop Loss Underwriter             2.7   \n",
       "4  Supply Chain Data Analyst (Day Shift) - New Be...             3.5   \n",
       "\n",
       "            company_name  tot_employment_in_state  Annual mean wage(2)  \\\n",
       "0   The Insurance Center                   3090.0             105250.0   \n",
       "1  Net Health Shops, LLC                   3090.0             105250.0   \n",
       "2        UW Credit Union                   3090.0             105250.0   \n",
       "3   The Insurance Center                   3090.0             105250.0   \n",
       "4            DB SCHENKER                   3090.0             105250.0   \n",
       "\n",
       "   Annual median wage(2)  total_new_post_rat  range_avg_to_mean_ratio  \\\n",
       "0               101850.0               515.0                 0.669834   \n",
       "1               101850.0               515.0                 1.199624   \n",
       "2               101850.0               515.0                 1.401425   \n",
       "3               101850.0               515.0                 0.679335   \n",
       "4               101850.0               515.0                 0.688836   \n",
       "\n",
       "   range_avg_to_median_ratio   labels  \n",
       "0                   0.692194  midwest  \n",
       "1                   1.239670  midwest  \n",
       "2                   1.448208  midwest  \n",
       "3                   0.702013  midwest  \n",
       "4                   0.711831  midwest  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('explored_data_v1.csv')\n",
    "\n",
    "df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b8ae3",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e4b38266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_salary_output(salary, low_salary, rat , df):\n",
    "    west_cit_list = [x for x in df.loc[df['labels']== 'west', 'city'].value_counts()[:7].index]\n",
    "    east_cit_list = [x for x in df.loc[df['labels']== 'east', 'city'].value_counts()[:7].index]\n",
    "    midwest_cit_list = [x for x in df.loc[df['labels']== 'midwest', 'city'].value_counts()[:7].index]\n",
    "    south_cit_list = [x for x in df.loc[df['labels']== 'south', 'city'].value_counts()[:7].index]\n",
    "    \n",
    "    \n",
    "    west_comp_list = [x for x in df.loc[df['labels']== 'west', 'company_name'].value_counts()[:7].index]\n",
    "    east_comp_list = [x for x in df.loc[df['labels']== 'east', 'company_name'].value_counts()[:7].index]\n",
    "    midwest_comp_list = [x for x in df.loc[df['labels']== 'midwest', 'company_name'].value_counts()[:7].index]\n",
    "    south_comp_list = [x for x in df.loc[df['labels']== 'south', 'company_name'].value_counts()[:7].index]\n",
    "    remote_comp_list = [x for x in df.loc[df['labels']== 'remote', 'company_name'].value_counts()[:7].index]\n",
    "    \n",
    "    west_job_list = [x for x in df.loc[df['labels']== 'west', 'Job Title'].value_counts()[:10].index]\n",
    "    east_job_list = [x for x in df.loc[df['labels']== 'east', 'Job Title'].value_counts()[:10].index]\n",
    "    midwest_job_list = [x for x in df.loc[df['labels']== 'midwest', 'Job Title'].value_counts()[:10].index]\n",
    "    south_job_list = [x for x in df.loc[df['labels']== 'south', 'Job Title'].value_counts()[:10].index]\n",
    "    remote_job_list = [x for x in df.loc[df['labels']== 'remote', 'Job Title'].value_counts()[:10].index]\n",
    "    \n",
    "    west = [west_cit_list, west_comp_list, west_job_list]\n",
    "    east = [east_cit_list, east_comp_list, east_job_list]\n",
    "    midwest = [midwest_cit_list, midwest_comp_list, midwest_job_list]\n",
    "    south = [south_cit_list, south_comp_list, south_job_list]\n",
    "    remote = [remote_comp_list, remote_job_list]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df = df.drop(columns=['state','city','Job Title','company_name'])\n",
    "    \n",
    "    dum_df = pd.get_dummies(df['labels'])\n",
    "\n",
    "\n",
    "    dummed_df = pd.concat([df, dum_df],axis=1)\n",
    "\n",
    "    dummed_df = dummed_df.drop(columns='labels')\n",
    "\n",
    "\n",
    "    features = list(dummed_df.columns[dummed_df.columns != 'west'])\n",
    "    features.remove('south')\n",
    "    features.remove('east')\n",
    "    features.remove('midwest')\n",
    "    features.remove('remote')\n",
    "\n",
    "    X = dummed_df[features]\n",
    "\n",
    "    y = dummed_df[['west','east','south','midwest','remote']]\n",
    "\n",
    "    \n",
    "    RFC = RandomForestClassifier(n_estimators=500, criterion='entropy')\n",
    "    res = RFC.fit(X, y)\n",
    "    \n",
    "    avg_diff = (np.mean(X['upper_salary'])-np.mean(X['lower_salary']))\n",
    "    \n",
    "    #lower_gen = salary - avg_diff\n",
    "    lower_gen = low_salary\n",
    "    \n",
    "    tot_emp_gen = np.median(X['tot_employment_in_state'])\n",
    "    \n",
    "    tot_post_gen = np.median(X['total_new_post_rat'])\n",
    "    \n",
    "    avg_mean_rat_gen = rat\n",
    "    \n",
    "    avg_median_rat_gen = rat - (np.mean(X['range_avg_to_mean_ratio'])-np.mean(X['range_avg_to_median_ratio']))\n",
    "    \n",
    "    ann_mean_gen = (lower_gen + avg_diff/2)* avg_mean_rat_gen\n",
    "    \n",
    "    ann_med_gen = (lower_gen + avg_diff/2)* avg_median_rat_gen\n",
    "    \n",
    "    comp_rate_gen = np.median(X['Company Rating'])\n",
    "    \n",
    "    sal_x = [salary, lower_gen, comp_rate_gen, tot_emp_gen, ann_mean_gen, ann_med_gen, tot_post_gen, avg_mean_rat_gen, avg_median_rat_gen]\n",
    "\n",
    "    sal_x = np.array(sal_x)\n",
    "    sal_x = sal_x.reshape(1,-1)\n",
    "\n",
    "    y_pred = res.predict(sal_x)\n",
    "    \n",
    "    y_prob = res.predict_proba(sal_x)\n",
    "    print (y_prob)\n",
    "    \n",
    "    y_list = []\n",
    "    for x in y_pred:\n",
    "        for y in x:\n",
    "            y_list.append(y)\n",
    "            \n",
    "    if y_list == [1, 0, 0 ,0 ,0]:\n",
    "        return (f'best cities to look at: {[x for x in west[0]]}. best companies to look for: {[x for x in west[1]]}. job titles to look for:{[x for x in west[2]]}')\n",
    "    elif y_list == [0, 1, 0, 0 ,0]:\n",
    "        return (f'best cities to look at: {[x for x in east[0]]}. best companies to look for: {[x for x in east[1]]}. job titles to look for:{[x for x in east[2]]}')\n",
    "    elif y_list == [0, 0, 1, 0 ,0]:\n",
    "        return (f'best cities to look at: {[x for x in south[0]]}. best companies to look for: {[x for x in south[1]]}. job titles to look for:{[x for x in south[2]]}')\n",
    "    elif y_list == [0, 0, 0, 1 ,0]:\n",
    "        return (f'best cities to look at: {[x for x in midwest[0]]}. best companies to look for: {[x for x in midwest[1]]}. job titles to look for:{[x for x in midwest[2]]}')\n",
    "    elif y_list == [0, 0, 0, 0 ,1]:\n",
    "        return (f'best companies to look for: {[x for x in remote[0]]}. job titles to look for:{[x for x in remote[1]]}')\n",
    "    else:\n",
    "        ('readjust query')\n",
    "        \n",
    "        \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "bb30b0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.794, 0.206]]), array([[0.972, 0.028]]), array([[0.824, 0.176]]), array([[0.416, 0.584]]), array([[0.994, 0.006]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rfc_trial = rfc_salary_output(120000, 80000, .8, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e910b36",
   "metadata": {},
   "source": [
    "There is a 58% probability that the classification is Midwestern, which is well above the percentages for others (21%, 3%, 18%, 1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "70dd2424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cities to look at: ['Riverwoods', 'Chicago', 'Cincinnati', 'Minneapolis', 'Detroit', 'Columbus', 'Onalaska']. best companies to look for: ['Discover Financial Services', 'Inizio Engage', 'The Insurance Center', 'Home Chef', 'S&S Activewear LLC', 'NextGen Federal Systems', 'Gohagan & Company']. job titles to look for:['Associate Stop Loss Underwriter', 'Senior Data Scientist', 'Manager, Data Science (Remote)', 'Rapid Deployment Representative', 'Sr. Healthcare Data Analyst', 'Lead Data Science Analyst', 'Sr. Data Analyst, Finance', 'Modeler', 'Lead Data Science Analyst (multiple openings) - IHM', 'Data Engineers']\n"
     ]
    }
   ],
   "source": [
    "print (rfc_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba941cb",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "04384fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gb_salary_output(salary, low_salary, rat , df):\n",
    "    west_cit_list = [x for x in df.loc[df['labels']== 'west', 'city'].value_counts()[:7].index]\n",
    "    east_cit_list = [x for x in df.loc[df['labels']== 'east', 'city'].value_counts()[:7].index]\n",
    "    midwest_cit_list = [x for x in df.loc[df['labels']== 'midwest', 'city'].value_counts()[:7].index]\n",
    "    south_cit_list = [x for x in df.loc[df['labels']== 'south', 'city'].value_counts()[:7].index]\n",
    "    \n",
    "    \n",
    "    west_comp_list = [x for x in df.loc[df['labels']== 'west', 'company_name'].value_counts()[:7].index]\n",
    "    east_comp_list = [x for x in df.loc[df['labels']== 'east', 'company_name'].value_counts()[:7].index]\n",
    "    midwest_comp_list = [x for x in df.loc[df['labels']== 'midwest', 'company_name'].value_counts()[:7].index]\n",
    "    south_comp_list = [x for x in df.loc[df['labels']== 'south', 'company_name'].value_counts()[:7].index]\n",
    "    remote_comp_list = [x for x in df.loc[df['labels']== 'remote', 'company_name'].value_counts()[:7].index]\n",
    "    \n",
    "    west_job_list = [x for x in df.loc[df['labels']== 'west', 'Job Title'].value_counts()[:10].index]\n",
    "    east_job_list = [x for x in df.loc[df['labels']== 'east', 'Job Title'].value_counts()[:10].index]\n",
    "    midwest_job_list = [x for x in df.loc[df['labels']== 'midwest', 'Job Title'].value_counts()[:10].index]\n",
    "    south_job_list = [x for x in df.loc[df['labels']== 'south', 'Job Title'].value_counts()[:10].index]\n",
    "    remote_job_list = [x for x in df.loc[df['labels']== 'remote', 'Job Title'].value_counts()[:10].index]\n",
    "    \n",
    "    west = [west_cit_list, west_comp_list, west_job_list]\n",
    "    east = [east_cit_list, east_comp_list, east_job_list]\n",
    "    midwest = [midwest_cit_list, midwest_comp_list, midwest_job_list]\n",
    "    south = [south_cit_list, south_comp_list, south_job_list]\n",
    "    remote = [remote_comp_list, remote_job_list]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df = df.drop(columns=['state','city','Job Title','company_name'])\n",
    "    \n",
    "    features = list(df.columns[df.columns != 'labels'])\n",
    "\n",
    "    X = df[features]\n",
    "    y = df['labels']\n",
    "\n",
    "\n",
    "    \n",
    "    GB = GradientBoostingClassifier(learning_rate=.75, max_depth=3, max_features=4, n_estimators=50)\n",
    "    res = GB.fit(X, y)\n",
    "    \n",
    "    avg_diff = (np.mean(X['upper_salary'])-np.mean(X['lower_salary']))\n",
    "    \n",
    "    #lower_gen = salary - avg_diff\n",
    "    lower_gen = low_salary\n",
    "    \n",
    "    tot_emp_gen = np.median(X['tot_employment_in_state'])\n",
    "    \n",
    "    tot_post_gen = np.median(X['total_new_post_rat'])\n",
    "    \n",
    "    avg_mean_rat_gen = rat\n",
    "    \n",
    "    avg_median_rat_gen = rat - (np.mean(X['range_avg_to_mean_ratio'])-np.mean(X['range_avg_to_median_ratio']))\n",
    "    \n",
    "    ann_mean_gen = (lower_gen + avg_diff/2)* avg_mean_rat_gen\n",
    "    \n",
    "    ann_med_gen = (lower_gen + avg_diff/2)* avg_median_rat_gen\n",
    "    \n",
    "    comp_rate_gen = np.median(X['Company Rating'])\n",
    "    \n",
    "    sal_x = [salary, lower_gen, comp_rate_gen, tot_emp_gen, ann_mean_gen, ann_med_gen, tot_post_gen, avg_mean_rat_gen, avg_median_rat_gen]\n",
    "\n",
    "    sal_x = np.array(sal_x)\n",
    "    sal_x = sal_x.reshape(1,-1)\n",
    "\n",
    "    y_pred = res.predict(sal_x)\n",
    "    print (y_pred)\n",
    "    y_prob = res.predict_proba(sal_x)\n",
    "    print (y_prob)\n",
    "    \n",
    "    y_list = []\n",
    "    for x in y_pred:\n",
    "        for y in x:\n",
    "            y_list.append(y)\n",
    "            \n",
    "    if y_pred == ['west']:\n",
    "        return (f'best cities to look at: {[x for x in west[0]]}. best companies to look for: {[x for x in west[1]]}. job titles to look for:{[x for x in west[2]]}')\n",
    "    elif y_pred == ['east']:\n",
    "        return (f'best cities to look at: {[x for x in east[0]]}. best companies to look for: {[x for x in east[1]]}. job titles to look for:{[x for x in east[2]]}')\n",
    "    elif y_pred == ['south']:\n",
    "        return (f'best cities to look at: {[x for x in south[0]]}. best companies to look for: {[x for x in south[1]]}. job titles to look for:{[x for x in south[2]]}')\n",
    "    elif y_pred == ['midwest']:\n",
    "        return (f'best cities to look at: {[x for x in midwest[0]]}. best companies to look for: {[x for x in midwest[1]]}. job titles to look for:{[x for x in midwest[2]]}')\n",
    "    elif y_pred == ['remote']:\n",
    "        return (f'best companies to look for: {[x for x in remote[0]]}. job titles to look for:{[x for x in remote[1]]}')\n",
    "    else:\n",
    "        ('readjust query')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f0e457a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['midwest']\n",
      "[[3.89927766e-08 9.97115013e-01 3.40976201e-09 2.88469087e-03\n",
      "  2.54221839e-07]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gb_trial = gb_salary_output(130000, 110000, .8, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab53344",
   "metadata": {},
   "source": [
    "A 99.7% probability that the classification is the midwest region, with the probability of the classification being the others very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "839726e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cities to look at: ['Riverwoods', 'Chicago', 'Cincinnati', 'Minneapolis', 'Detroit', 'Columbus', 'Onalaska']. best companies to look for: ['Discover Financial Services', 'Inizio Engage', 'The Insurance Center', 'Home Chef', 'S&S Activewear LLC', 'NextGen Federal Systems', 'Gohagan & Company']. job titles to look for:['Associate Stop Loss Underwriter', 'Senior Data Scientist', 'Manager, Data Science (Remote)', 'Rapid Deployment Representative', 'Sr. Healthcare Data Analyst', 'Lead Data Science Analyst', 'Sr. Data Analyst, Finance', 'Modeler', 'Lead Data Science Analyst (multiple openings) - IHM', 'Data Engineers']\n"
     ]
    }
   ],
   "source": [
    "print (gb_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e195c43",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "The hyperparamter tuned gradient boosting model was able to classify my ideal economic region with much greater certainty than the random forest classifier (99.7% probability vs 58.4% probability).\n",
    "\n",
    "It seems that this gradient boosting model, despite its algorithmic complexity, is appropriate with data this size and a question at this level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323e1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
