{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd01d587",
   "metadata": {},
   "source": [
    "# Preprocessing and model training\n",
    "\n",
    "\n",
    "In this notebook, I will prepare my dataframes and then train several classification models off of them and compare results.\n",
    "\n",
    "Building off of two previous notebooks: (https://github.com/gisthuband/Capstone_2_DS_Job_Locator/blob/main/data_wrangle.ipynb and https://github.com/gisthuband/Capstone_2_DS_Job_Locator/blob/main/exploratory_data_analysis.ipynb), I have constructed and analyzed a dataframe containing information on data science jobs in 2024.\n",
    "\n",
    "I will use data to create a classification model, that will take my desired salary range and self perceived competitiveness in the job market, and use that to find the best locations and companies to apply for.\n",
    "\n",
    "The data was found using a kaggle dataset containing 500 job postings for the data science filed in 2024, and a BLS report generated using data science field statistics of 2023.\n",
    "\n",
    "Kaggle: https://www.kaggle.com/datasets/ritiksharma07/data-science-job-listings-from-glassdoor   \n",
    "\n",
    "BLS: https://data.bls.gov/oes/#/occInd/One%20occupation%20for%20multiple%20industries \n",
    "\n",
    "The samples' (the individual job postings) features will be their upper salary post, lower salary post, company rating, total data scientists in company's state, ratio of job posts to total data scientists in company state, annual mean wage of state, annual median wage of state, ratio of job post to annual mean wage, and ratio of job post to annual median wage.\n",
    "\n",
    "Each job will receive its label based on geographic region: west, midwest, south, east, or remote.\n",
    "\n",
    "The models will train based of off the numerical features as x and the regions as y.\n",
    "\n",
    "\n",
    "In the end, I will input my own desired salary range and my perceived competitiveness in the job market.  The salary range will correspond to the upper and lower salary features.  The perceived competitiveness will become the ratio of posting to state mean, which that in turn will be used to calculate the ratio of posting to state median.  These ratios will determine the state mean and median features in tandem with the inputted salary range.  The company rating, employment in state, and ratio of posts to employment will be automatically taken as their median values as to not overcomplicate the model.  From this input I will get the region label, and from this region label I can use the original dataframe and generate, the top posting cities in that region, along with the companies and the titles of roles accompanying those posts.\n",
    "\n",
    "This notebook will contain the following: a preprocessing class that will have the capacity to:\n",
    "\n",
    "1.) Produce a tidied up dataframe\n",
    "\n",
    "2.) Produce test and training version of data without one hot encoding for the labels\n",
    "\n",
    "3.) Produce a test and training version of data with one hot encoding for the labels\n",
    "\n",
    "4.) Produce a test and training version of data with one hot encoding for the labels and a standardized scaling\n",
    "\n",
    "5.) Produce a test and training version of data with one hot encoding for the labels and a max min scaling\n",
    "\n",
    "6.) Produce a mislabeled test and training labels without one hot encoding\n",
    "\n",
    "7.) Produce a mislabedl test and training labels with one hot encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331dc223",
   "metadata": {},
   "source": [
    "## Creation of Preprocessing Class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "723301e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bef909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self, df_path):\n",
    "        self.df_path = df_path\n",
    "    \n",
    "    \n",
    "    def refine(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['state','city','Job Title','company_name'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def regular(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['state','city','Job Title','company_name'])\n",
    "        \n",
    "        ready_df = df\n",
    "        \n",
    "        features = list(ready_df.columns[ready_df.columns != 'labels'])\n",
    "\n",
    "        X = ready_df[features]\n",
    "        \n",
    "        y = ready_df['labels']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1, stratify=y)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def dummied(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['state','city','Job Title','company_name'])\n",
    "        \n",
    "        dum_df = pd.get_dummies(df['labels'])        \n",
    "        \n",
    "        dummed_df = pd.concat([df, dum_df],axis=1)\n",
    "\n",
    "        dummed_df = dummed_df.drop(columns='labels')\n",
    "        \n",
    "        features = list(dummed_df.columns[dummed_df.columns != 'west'])\n",
    "        \n",
    "        features.remove('south')\n",
    "        \n",
    "        features.remove('east')\n",
    "        \n",
    "        features.remove('midwest')\n",
    "        \n",
    "        features.remove('remote')\n",
    "        \n",
    "        X = dummed_df[features]\n",
    "\n",
    "        y = dummed_df[['west','east','south','midwest','remote']]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def standard_scaled(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['state','city','Job Title','company_name'])\n",
    "        \n",
    "        dum_df = pd.get_dummies(df['labels'])        \n",
    "        \n",
    "        dummed_df = pd.concat([df, dum_df],axis=1)\n",
    "\n",
    "        dummed_df = dummed_df.drop(columns='labels')\n",
    "        \n",
    "        features = list(dummed_df.columns[dummed_df.columns != 'west'])\n",
    "        \n",
    "        features.remove('south')\n",
    "        \n",
    "        features.remove('east')\n",
    "        \n",
    "        features.remove('midwest')\n",
    "        \n",
    "        features.remove('remote')\n",
    "        \n",
    "        X = dummed_df[features]\n",
    "\n",
    "        y = dummed_df[['west','east','south','midwest','remote']]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "        \n",
    "        s_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        \n",
    "        X_train=s_scaler.transform(X_train)\n",
    "        \n",
    "        X_test=s_scaler.transform(X_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def maxmin_scaled(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['state','city','Job Title','company_name'])\n",
    "        \n",
    "        dum_df = pd.get_dummies(df['labels'])        \n",
    "        \n",
    "        dummed_df = pd.concat([df, dum_df],axis=1)\n",
    "\n",
    "        dummed_df = dummed_df.drop(columns='labels')\n",
    "        \n",
    "        features = list(dummed_df.columns[dummed_df.columns != 'west'])\n",
    "        \n",
    "        features.remove('south')\n",
    "        \n",
    "        features.remove('east')\n",
    "        \n",
    "        features.remove('midwest')\n",
    "        \n",
    "        features.remove('remote')\n",
    "        \n",
    "        X = dummed_df[features]\n",
    "\n",
    "        y = dummed_df[['west','east','south','midwest','remote']]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "        \n",
    "        X_train=preprocessing.minmax_scale(X_train)\n",
    "        \n",
    "        X_test=preprocessing.minmax_scale(X_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def random_label_regular(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['state','city','Job Title','company_name'])\n",
    "        \n",
    "        ready_df = df\n",
    "        \n",
    "        rng = np.random.default_rng()\n",
    "        \n",
    "        rand_labels = rng.integers(low=0, high=5, size=len(ready_df)) \n",
    "        \n",
    "        region_labels = ['west','east','south','midwest','remote']\n",
    "        \n",
    "        random_label_fin = list(map(lambda x: region_labels[x], rand_labels))\n",
    "        \n",
    "        ready_df['random_labels'] = random_label_fin\n",
    "        \n",
    "        ready_df = ready_df.drop(columns='labels')\n",
    "        \n",
    "        features = list(ready_df.columns[ready_df.columns != 'random_labels'])\n",
    "\n",
    "        X = ready_df[features]\n",
    "        \n",
    "        y = ready_df['random_labels']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1, stratify=y)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def random_label_dum(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['state','city','Job Title','company_name'])\n",
    "        \n",
    "        rng = np.random.default_rng()\n",
    "        \n",
    "        rand_labels = rng.integers(low=0, high=5, size=len(df)) \n",
    "        \n",
    "        region_labels = ['west','east','south','midwest','remote']\n",
    "        \n",
    "        random_label_fin = list(map(lambda x: region_labels[x], rand_labels))\n",
    "        \n",
    "        df['random_labels'] = random_label_fin\n",
    "        \n",
    "        dum_df = pd.get_dummies(df['random_labels'])        \n",
    "        \n",
    "        dummed_df = pd.concat([df, dum_df],axis=1)\n",
    "\n",
    "        dummed_df = dummed_df.drop(columns=['labels','random_labels'])\n",
    "        \n",
    "        features = list(dummed_df.columns[dummed_df.columns != 'west'])\n",
    "        \n",
    "        features.remove('south')\n",
    "        \n",
    "        features.remove('east')\n",
    "        \n",
    "        features.remove('midwest')\n",
    "        \n",
    "        features.remove('remote')\n",
    "        \n",
    "        X = dummed_df[features]\n",
    "\n",
    "        y = dummed_df[['west','east','south','midwest','remote']]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177322f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_regular = preprocess.regular('explored_data_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dee2f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85        west\n",
      "147    midwest\n",
      "251     remote\n",
      "35        east\n",
      "42       south\n",
      "        ...   \n",
      "114      south\n",
      "130      south\n",
      "134    midwest\n",
      "186      south\n",
      "75        west\n",
      "Name: labels, Length: 69, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (trial_regular[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "427ea0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_fake = preprocess.random_label_regular('explored_data_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ab6d09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109     remote\n",
      "331     remote\n",
      "211     remote\n",
      "151       west\n",
      "342    midwest\n",
      "        ...   \n",
      "90      remote\n",
      "111    midwest\n",
      "246     remote\n",
      "227    midwest\n",
      "281       east\n",
      "Name: random_labels, Length: 69, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (trial_fake[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
